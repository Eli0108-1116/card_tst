import os
import cv2
import numpy as np
import faiss
import re
import html
from tqdm import tqdm
from backend.image_processing import load_or_build_cache

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
INFO_DIR = os.path.join(BASE_DIR, "data", "cards_info_updated")
INDEX_PATH = os.path.join(BASE_DIR, "data", "cache", "all.index")
DESC_FILE = os.path.join(BASE_DIR, "data", "cache", "all.npy")


def build_or_load_index(des_list, desc_dim):
    if os.path.exists(INDEX_PATH):
        return faiss.read_index(INDEX_PATH)
    quantizer = faiss.IndexFlatL2(desc_dim)
    index = faiss.IndexIVFPQ(quantizer, desc_dim, 500, 24, 8)
    index.train(des_list)
    index.add(des_list)
    faiss.write_index(index, INDEX_PATH)
    print("âœ… å»ºç«‹æ–°ç´¢å¼•å®Œæˆ")
    return index


def match_single_crop(des1, index, descs, names):
    D, I = index.search(des1.astype('float32'), 2)
    good_per_img = [[] for _ in descs]
    boundaries = np.cumsum([len(d) for d in descs])
    for qi in range(len(des1)):
        d0, d1 = D[qi]
        if d0 < 0.9 * d1:
            tr = int(I[qi, 0])
            idx = np.searchsorted(boundaries, tr, side='right')
            start = boundaries[idx - 1] if idx > 0 else 0
            local = tr - start
            good_per_img[idx].append(cv2.DMatch(qi, local, d0))

    best_idx = max(range(len(good_per_img)), key=lambda i: len(good_per_img[i]), default=None)
    if best_idx is None or len(good_per_img[best_idx]) < 2:
        return None

    return names[best_idx]


def read_info(matched_name, category_en="all"):
    import html, re, os

    card_id = os.path.splitext(matched_name)[0].zfill(8)

    matches = [
        fname for fname in os.listdir(INFO_DIR)
        if fname.startswith(card_id) and fname.lower().endswith(".txt")
    ]
    if not matches:
        print(f"âš ï¸ æ‰¾åˆ°ç›¸ä¼¼å¡ç‰‡ {matched_name}ï¼Œä½†ç¼ºå°‘å°æ‡‰è³‡è¨Šæª”æ¡ˆ")
        return None

    info_file = os.path.join(INFO_DIR, matches[0])
    print(f"ğŸ” åŒ¹é…è³‡è¨Šæª”æ¡ˆï¼š{info_file}")

    with open(info_file, encoding="utf-8") as f:
        info = f.read()

    # ğŸ”§ è‡ªå‹•çµ„å‡º GitHub Pages åœ–ç‰‡ç¶²å€
    BASE_IMAGE_URL = "https://salix5.github.io/query-data/pics"
    img_url = f"{BASE_IMAGE_URL}/{int(card_id)}.jpg"
    img_tag = f'<img src="{img_url}" alt="å¡åœ–" style="max-width:300px; margin:5px;" />'

    # ğŸ” ç§»é™¤èˆŠåœ–ç‰‡ç¶²å€ã€æ›è¡Œæ ¼å¼åŒ–
    info = re.sub(r'https?://[^\s"]+\.(?:jpg|jpeg|png)', '', info)
    info = html.escape(info, quote=False).replace("\n", " <br>")

    return f"{img_tag}<br>{info}"


def process_multi_image(image_bytes_list, category_en="all"):
    paths, names, kp_attrs, descs, all_desc = load_or_build_cache("all")
    index = build_or_load_index(all_desc, descs[0].shape[1])

    category_map = {
        'spell': 'é­”æ³•',
        'trap': 'é™·é˜±',
        'effect': 'æ•ˆæœ',
        'normal': 'é€šå¸¸',
        'ritual': 'å„€å¼',
        'fusion': 'èåˆ',
        'synchro': 'åŒæ­¥',
        'xyz': 'è¶…é‡',
        'link': 'é€£çµ',
        'pendulum': 'éˆæ“º',
        'all': 'å…¨éƒ¨'
    }
    category_ch = category_map.get(category_en, category_en)

    results = []
    sift = cv2.SIFT_create()

    for img_data in tqdm(image_bytes_list, desc="è™•ç†æ¯å¼µè£åˆ‡åœ–ç‰‡"):
        img = cv2.imdecode(np.frombuffer(img_data, np.uint8), cv2.IMREAD_COLOR)
        if img is None:
            continue
        kp, des = sift.detectAndCompute(img, None)
        if des is None or len(kp) == 0:
            continue

        matched_name = match_single_crop(des, index, descs, names)
        if matched_name:
            # è®“ read_info æ ¹æ“š category_en æ˜¯ä¸æ˜¯ all è‡ªå‹•æ¨è«–åˆ†é¡
            info_html = read_info(matched_name, category_en)
            if info_html:
                results.append(info_html)

    if not results:
        return "<p>âŒ æ²’æœ‰è¾¨è­˜å‡ºä»»ä½•å¡ç‰‡</p>"

    result_text = f"<p>è¾¨è­˜å‡º {len(results)} å¼µå¡ç‰‡ï¼š</p>"
    for r in results:
        result_text += r + "<hr>"

    return result_text


